---
title: "Benchmarking Multi-Agent Preference-based Reinforcement Learning for Human-AI Teaming"
collection: publications
permalink: /publication/2023-12-21-Benchmarking-Multi-Agent-Preference-based-Reinforcement-Learning-for-Human-AI-Teaming
# excerpt: 'In this work, we investigate the potential of Large Language Models (LLMs) to serve as effective human proxies by capturing human preferences in the context of collaboration with AI agents. Focusing on two key aspects of human preferences- explicability and sub-task specification in team settings - we explore LLMs’ ability to not only model mental states but also understand human reasoning processes. By developing scenarios where optimal AI performance relies on modeling human mental states and reasoning, our investigation involving two different preference types and a user study (with 17 participants) contributes valuable insights into the suitability of LLMs as “Preference Proxies” in various human-AI applications, paving the way for future research on the integration of AI agents with human users in Human-Aware AI tasks'
date: 2023-12-21
venue: 'arXiv Pre-print'
paperurl: 'https://arxiv.org/pdf/2312.14292'
citation: 'Bhambri, Siddhant, Mudit Verma, Anil Murthy, and Subbarao Kambhampati. "Benchmarking Multi-Agent Preference-based Reinforcement Learning for Human-AI Teaming." arXiv preprint arXiv:2312.14292 (2023).'
---
**Abstract**: n this work, we investigate the potential of Large Language Models (LLMs) to serve as effective human proxies by capturing human preferences in the context of collaboration with AI agents. Focusing on two key aspects of human preferences- explicability and sub-task specification in team settings - we explore LLMs’ ability to not only model mental states but also understand human reasoning processes. By developing scenarios where optimal AI performance relies on modeling human mental states and reasoning, our investigation involving two different preference types and a user study (with 17 participants) contributes valuable insights into the suitability of LLMs as “Preference Proxies” in various human-AI applications, paving the way for future research on the integration of AI agents with human users in Human-Aware AI tasks

[Download paper here](https://github.com/sbhambr1/siddhantbhambri.github.io/raw/master/files/Contrastively%20Learning%20Visual%20Attention%20as%20Affordance%20Cues%20from%20Demonstrations%20for%20Robotic%20Grasping.pdf)

<!-- Recommended citation: Y. Zha, S. Bhambri and L. Guan, "Contrastively Learning Visual Attention as Affordance Cues from Demonstrations for Robotic Grasping," 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021, pp. 7835-7842, doi: 10.1109/IROS51168.2021.9636760. -->