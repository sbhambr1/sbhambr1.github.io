---
title: "Exploiting Unlabeled Data for Feedback Efficient Human Preference based Reinforcement Learning"
collection: publications
permalink: /publication/2023-05-24-Exploiting-Unlabeled-Data-for-Feedback-Efficient-Human-Preference-based-Reinforcement-Learning
excerpt: 'Preference Based Reinforcement Learning has shown much promise for utilizing human binary feedback on queried trajectory pairs to recover the underlying reward model of the Human in the Loop (HiL). While works have attempted to better utilize the queries made to the human, in this work we make two observations about the unlabeled trajectories collected by the agent and propose two corresponding loss functions that ensure participation of unlabeled trajectories in the reward learning process, and structure the embedding space of the reward model such that it reflects the structure of state space with respect to action distances. We validate the proposed method on one locomotion domain and one robotic manipulation task and compare with the state-of-the-art baseline PEBBLE. We further present an ablation of the proposed loss components across both the domains and find that not only each of the loss components perform better than the baseline, but the synergic combination of the two has much better reward recovery and human feedback sample efficiency.'
date: 2023-05-24
venue: 'The AAAI 2023 Workshop on Representation Learning for Responsible Human-Centric AI (R2HCAI), and ICML 2023 - Many Facets of Preference Learning Workshop'
paperurl: 'https://arxiv.org/abs/2302.08738'
citation: 'Verma, Mudit, Siddhant Bhambri, and Subbarao Kambhampati. "Exploiting Unlabeled Data for Feedback Efficient Human Preference based Reinforcement Learning." arXiv preprint arXiv:2302.08738 (2023).'
---
**Abstract**: Preference Based Reinforcement Learning has shown much promise for utilizing human binary feedback on queried trajectory pairs to recover the underlying reward model of the Human in the Loop (HiL). While works have attempted to better utilize the queries made to the human, in this work we make two observations about the unlabeled trajectories collected by the agent and propose two corresponding loss functions that ensure participation of unlabeled trajectories in the reward learning process, and structure the embedding space of the reward model such that it reflects the structure of state space with respect to action distances. We validate the proposed method on one locomotion domain and one robotic manipulation task and compare with the state-of-the-art baseline PEBBLE. We further present an ablation of the proposed loss components across both the domains and find that not only each of the loss components perform better than the baseline, but the synergic combination of the two has much better reward recovery and human feedback sample efficiency.

[Download paper here](https://github.com/sbhambr1/siddhantbhambri.github.io/raw/master/files/Contrastively%20Learning%20Visual%20Attention%20as%20Affordance%20Cues%20from%20Demonstrations%20for%20Robotic%20Grasping.pdf)

<!-- Recommended citation: Y. Zha, S. Bhambri and L. Guan, "Contrastively Learning Visual Attention as Affordance Cues from Demonstrations for Robotic Grasping," 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021, pp. 7835-7842, doi: 10.1109/IROS51168.2021.9636760. -->